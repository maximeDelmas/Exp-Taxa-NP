{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFpo-JT3ddww"
      },
      "source": [
        "# Prompting strategies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4lClWR9_dpzA"
      },
      "outputs": [],
      "source": [
        "# load libraries\n",
        "import os\n",
        "import gc\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers.file_utils import is_torch_available\n",
        "from scipy.spatial import distance\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G8HgD9VQ0a8",
        "outputId": "2df30993-a5b1-4eb3-ff9b-5681f18d52c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-OuE-wr9DqKh"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.RandomState(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YDGj-n0dycX",
        "outputId": "509bda95-f99e-4b45-b781-4fe5d363e826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   fungi_id                 fungi_name       family_name  pubchem_id  \\\n",
            "0    119834       Alternaria alternata     Pleosporaceae     5360741   \n",
            "1    257047  Cephalosporium aphidicola   Cordycipitaceae      457964   \n",
            "2    237604        Cordyceps militaris   Cordycipitaceae        6303   \n",
            "3    284309          Aspergillus niger    Aspergillaceae     5748546   \n",
            "4    815927     Albifimbria verrucaria  Stachybotryaceae     6326658   \n",
            "\n",
            "                      chem_name  nb_ref  y  \n",
            "0  Alternariol monomethyl ether      11  1  \n",
            "1                   Aphidicolin      10  1  \n",
            "2                    Cordycepin       6  1  \n",
            "3                  Flavasperone       6  1  \n",
            "4                  Verrucarin A       5  1  \n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "dir = \"../../data/np/discriminant-analysis/\" # to change according to the file system\n",
        "data_file = \"dataset.tsv\"\n",
        "\n",
        "data = pd.read_csv(os.path.join(dir, data_file), sep=\"\\t\")\n",
        "\n",
        "# Show data\n",
        "print(data.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List of all the selected pre-trained Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y5g3rI3sfSqX"
      },
      "outputs": [],
      "source": [
        "models = {'ChemicalBERT':'recobo/chemical-bert-uncased',\n",
        "    'BioBERT':'dmis-lab/biobert-base-cased-v1.2',\n",
        "\n",
        "    'BERT':'bert-base-uncased',\n",
        "    'BERT-large': 'bert-large-cased-whole-word-masking',\n",
        "\n",
        "    'RoBERTa':'roberta-base',\n",
        "    'RoBERTa-large':'roberta-large',\n",
        "\n",
        "    'BigBird-RoBERTa-large':'google/bigbird-roberta-large',\n",
        "\n",
        "    'Muppet-RoBERTa-large':'facebook/muppet-roberta-large',\n",
        "\n",
        "    'PubMedBERT-full':'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
        "    'PubMedBERT':'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
        "\n",
        "    'Clinical-BigBird':'yikuan8/Clinical-BigBird',\n",
        "    'Clinical-Longformer':'yikuan8/Clinical-Longformer'\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3gTMxYoLhjXv"
      },
      "source": [
        "## Utility functions and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fYzJ475xhnlh"
      },
      "outputs": [],
      "source": [
        "# 1) Set-up manual prompts verbaliser\n",
        "\n",
        "class ManualPromptDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Dataset generator for task1.\n",
        "  Generate manual prompt for each pairs in the dataset.\n",
        "  Parameters are:\n",
        "    - the dataset\n",
        "    - the model to select the AutoTokenizer\n",
        "    - a prompt template\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data, tokenizer, template, max_length):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.template = template\n",
        "    self.max_length = max_length\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.data.shape[0]\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # Get data\n",
        "    chemical_name = self.data.loc[index, \"chem_name\"]\n",
        "    fungi_name = self.data.loc[index, \"fungi_name\"]\n",
        "    # fill and tokenize prompt template\n",
        "    filled_template = self.template.format(compound=chemical_name, mask=self.tokenizer.mask_token, fungi=fungi_name)\n",
        "    tokenized = self.tokenizer(filled_template, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    \n",
        "    for k in tokenized.keys():\n",
        "      tokenized[k] = torch.squeeze(tokenized[k])\n",
        "\n",
        "    # remove \n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def get_proba_matrix(dataloader, model, vocab_size, mask_token_id, device):\n",
        "  \"\"\"\n",
        "  Evaluate the performances of each models and each template \n",
        "  \"\"\"\n",
        "\n",
        "  # Init token expected counts\n",
        "  proba_matrix = np.array([]).reshape(0, vocab_size)\n",
        "\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    print(\"    - batch: \" + str(step))\n",
        "\n",
        "    n, m = batch[\"input_ids\"].shape\n",
        "\n",
        "    # save input ids in classic device before using gpu (in case)\n",
        "    input_ids = batch[\"input_ids\"].clone()\n",
        "    \n",
        "    inputs = batch.to(device)\n",
        "    \n",
        "    # send batch to model\n",
        "    out = model(**inputs)\n",
        "\n",
        "    # get proba for the masked token\n",
        "    # 1- recover indexes of the masked token\n",
        "    masked_token_indexes = (input_ids == mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "    # 2- transform indexes so that we can extract the correspond line in the 3D tensor. The idea is to transform the 3D tensor (batch_size, seq_length, hidden_size) en un 2D tensor (batch_size * seq_length, hidden_size).\n",
        "    # Ensuite, on a plus qu'a incrémenter les index initiaux de 64 en plus pour chaque ligne de tel sorte à ce qu'il corresponde dans la matrice 2D.\n",
        "    masked_token_indexes = masked_token_indexes + torch.arange(0, m * n, m)\n",
        "\n",
        "    # Compute proba\n",
        "    proba_masked_tokens = torch.nn.functional.softmax(out.logits[:, :, 0:vocab_size].view(-1, vocab_size)[masked_token_indexes], dim=1)\n",
        "\n",
        "    # concat in proba matrix\n",
        "    proba_matrix = np.concatenate((proba_matrix, proba_masked_tokens.detach().cpu().numpy()), axis=0)\n",
        "\n",
        "  return proba_matrix\n",
        "\n",
        "\n",
        "\n",
        "def get_expected_top_k(dataloader, model, vocab_size, mask_token_id, k, device):\n",
        "  \"\"\"\n",
        "  Get the top k expected tokens\n",
        "  \"\"\"\n",
        "\n",
        "  expected_count_matrix = np.zeros(vocab_size)\n",
        "\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    print(\"    - batch: \" + str(step))\n",
        "\n",
        "    n, m = batch[\"input_ids\"].shape\n",
        "\n",
        "    # save input ids in classic device before using gpu (in case)\n",
        "    input_ids = batch[\"input_ids\"].clone()\n",
        "    \n",
        "    inputs = batch.to(device)\n",
        "    \n",
        "    # send batch to model\n",
        "    out = model(**inputs)\n",
        "\n",
        "    # get proba for the masked token\n",
        "    # 1- recover indexes of the masked token\n",
        "    masked_token_indexes = (input_ids == mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "    # 2- transform indexes so that we can extract the correspond line in the 3D tensor. The idea is to transform the 3D tensor (batch_size, seq_length, hidden_size) en un 2D tensor (batch_size * seq_length, hidden_size).\n",
        "    # Ensuite, on a plus qu'a incrémenter les index initiaux de 64 en plus pour chaque ligne de tel sorte à ce qu'il corresponde dans la matrice 2D.\n",
        "    masked_token_indexes = masked_token_indexes + torch.arange(0, m * n, m)\n",
        "\n",
        "    # Compute proba\n",
        "    proba_masked_tokens = torch.nn.functional.softmax(out.logits[:, :, 0:vocab_size].view(-1, vocab_size)[masked_token_indexes], dim=1)\n",
        "\n",
        "    # concat in proba matrix\n",
        "    expected_count_matrix += np.sum(proba_masked_tokens.detach().cpu().numpy(), axis=0)\n",
        "  \n",
        "  top_k_indexes = np.argsort(expected_count_matrix)[::-1][:k]\n",
        "  top_k_values = expected_count_matrix[top_k_indexes]\n",
        "\n",
        "  return top_k_indexes, top_k_values\n",
        "\n",
        "\n",
        "  \n",
        "def get_top_1_distribution(input_matrix):\n",
        "  \n",
        "  # init\n",
        "  n, m = input_matrix.shape\n",
        "  counts = np.zeros(m)\n",
        "  \n",
        "  # get top 1 for each example\n",
        "  for i in range(n):\n",
        "    top1_index = np.argmax(input_matrix[i])\n",
        "    counts[top1_index] += 1\n",
        "  \n",
        "  # transform as probs\n",
        "  counts = counts / np.sum(counts)\n",
        "  return counts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_JS_divergences(m1, m2, n_sample):\n",
        "  \"\"\"\n",
        "  m1 and m2 are matrix of words probability od dim (n x V) where n in the numner of samples and V the vocabulary size.\n",
        "  \"\"\"\n",
        "  \n",
        "  if not m1.shape == m2.shape:\n",
        "    print(\"m1 and n2 must have the same dimensions\")\n",
        "    return False\n",
        "  \n",
        "  n, m = m1.shape\n",
        "  JS_divergences = np.empty(n_sample)\n",
        "  \n",
        "  for k in range(n_sample):\n",
        "\n",
        "    i = random.choice(range(n))\n",
        "    j = random.choice(range(n))\n",
        "\n",
        "    JS_divergences[k] = distance.jensenshannon(m1[i], m2[j])\n",
        "  \n",
        "  return JS_divergences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8br8zWld-MM4"
      },
      "source": [
        "### Les templates\n",
        "\n",
        "-  On a des templates pour faire de la sentiment analysis: on demande de compléter par un verbe ou un adjectif qui devrait être représentatif de la nature du statement : vrai ou faux\n",
        "\n",
        "-  Les templates dits de prédiction demande quant à eux de compléter la phrase avec un composé chimique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KXYKb3WNSxVW"
      },
      "outputs": [],
      "source": [
        "templates_eval_1 = [\n",
        "  'Compound {compound} was {mask} from fungus {fungi} with the antimicrobial guided isolation procedure.',\n",
        "  '{compound} was {mask} from {fungi} with the antimicrobial guided isolation procedure.',\n",
        "  'Compound {compound} was {mask} obtained from fungus {fungi} with the antimicrobial guided isolation procedure.',\n",
        "  '{compound} was {mask} obtained from {fungi} with the antimicrobial guided isolation procedure.',          \n",
        "  'Fungus {fungi} showed {mask} {compound} activity.',\n",
        "  '{fungi} showed {mask} {compound} activity.',\n",
        "  'Fungus {fungi} {mask} {compound} activity.',\n",
        "  '{fungi} {mask} {compound} activity.',\n",
        "  'Fungus {fungi} {mask} compound {compound}.',\n",
        "  'Authors {mask} a natural product called {compound} from the {fungi}.',\n",
        "  'A strain {fungi} was isolated as a {mask} {compound} producer.',                 \n",
        "  'A strain {fungi} was {mask} as a high {compound} producer.',\n",
        "  'Compound {compound} is produced by fungus {fungi}. It is {mask}.']\n",
        "\n",
        "templates_eval_2 = [\n",
        "  'Among isolated chemical compounds, {compound} presented {mask} antimicrobial activities.',\n",
        "  'Compound {compound} showed {mask} growth inhibition on strains.',\n",
        "  '{compound} showed {mask} growth inhibition on strains.',\n",
        "  'Compound {compound} showed {mask} growth inhibition on drug-resistant pathogenic strains.',\n",
        "  '{compound} showed {mask} growth inhibition on drug-resistant pathogenic strains.',\n",
        "  'Compound {compound} {mask} the growth of the strains.',\n",
        "  '{compound} {mask} the growth of the strains.',\n",
        "  'Compound {compound} has antibiotic activity. It is {mask}']\n",
        "\n",
        "test_models = {'BioBERT':'dmis-lab/biobert-base-cased-v1.2'}\n",
        "\n",
        "test_templates_task1_sentiment = ['Compound {compound} was {mask} from fungus {fungi} with the antimicrobial guided isolation procedure.',\n",
        "  '{compound} was {mask} from {fungi} with the antimicrobial guided isolation procedure.']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RlEFz8cYLARm"
      },
      "source": [
        "## Run evaluation on all combinations and save the results\n",
        "\n",
        "* Hypothesis: If the models can distinguish true and false statements, then they should exhibit different distributions of predicted tokens between a true and a false example, while having a relatively conserved distribution between pairs of true, or, false examples.\n",
        "\n",
        "* The Cliff's Delta indicates how often the observed divergence (*Jensen-Shanon*) of predicted tokens between a positive and a negative example, is higher than the divergence between a pair of positive, or, negative examples.\n",
        "\n",
        "* We chose the *Jensen-Shanon* measure as it is a symetric version of the KL-divergence used to measure dissimilarity between two probability distributions.\n",
        "\n",
        "When comparing 2 variables $X$ and $Y$ of respective sizes $m$ and $n$:\n",
        "\n",
        "$$Delta = \\frac{1}{mn} \\sum_{i=1}^m \\sum_{j=1}^n \\delta_{ij}$$\n",
        "\n",
        "$$\n",
        "\\delta_{ij} = \n",
        "  \\begin{cases}\n",
        "  1 \\text{ si } x_i > y_j \\\\\n",
        "  -1 \\text{ si } x_i < y_j \\\\\n",
        "  0 sinon\n",
        "  \\end{cases}\n",
        "$$ \n",
        "\n",
        "* This tpe of experiment is also reproduced by comparing the correlation of the top1 predicted tokens in true and false statements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NBOC0gutK_0k"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_JS_divergences_on_dataset(positive_pairs, negative_pairs, models_set, templates_set, n_sample, dir):\n",
        "\n",
        "  proba_matrix_positives = np.array([])\n",
        "  proba_matrix_negatives = np.array([])\n",
        "\n",
        "  print(\"Device: \" + str(device))\n",
        "  batch_size = 64\n",
        "  max_length = 64\n",
        "\n",
        "  JS_divergence_table = pd.DataFrame()\n",
        "\n",
        "  # For each model\n",
        "  for model_name, model_ref in models_set.items():\n",
        "    \n",
        "    print(\"Treating model \" + model_name)\n",
        "\n",
        "    # load model and tokenizer\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_ref)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ref, use_fast=True)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    mask_token_id = tokenizer.mask_token_id\n",
        "\n",
        "    # just inference, no backward needed\n",
        "    with torch.no_grad(): \n",
        "\n",
        "      for template_index in range(len(templates_set)):\n",
        "\n",
        "        print(\" - Template: \" + str(template_index))\n",
        "        \n",
        "        template = templates_set[template_index]\n",
        "\n",
        "        # load data for POSTIVE pairs\n",
        "        dataset = ManualPromptDataset(data=positive_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n",
        "        dataloader_positive = DataLoader(dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "        # get proba matrix for positive examples\n",
        "        proba_matrix_positives = get_proba_matrix(dataloader_positive, model, vocab_size, mask_token_id, device)\n",
        "\n",
        "        # load data for NEGATIVE pairs\n",
        "        dataset = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n",
        "        dataloader_negative = DataLoader(dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "        # get proba matrix for negative examples\n",
        "        proba_matrix_negatives = get_proba_matrix(dataloader_negative, model, vocab_size, mask_token_id, device)\n",
        "\n",
        "        # Compute Positive x Positive JS divergence:\n",
        "        pos_pos_JS_divergences = compute_JS_divergences(proba_matrix_positives, proba_matrix_positives, n_sample)\n",
        "\n",
        "        # Compute Negative x Negative JS divergence:\n",
        "        neg_neg_JS_divergences = compute_JS_divergences(proba_matrix_negatives, proba_matrix_negatives, n_sample)\n",
        "\n",
        "        # Compute Postive x Negative JS divergence:\n",
        "        pos_neg_JS_divergences = compute_JS_divergences(proba_matrix_positives, proba_matrix_negatives, n_sample)\n",
        "        n = len(pos_neg_JS_divergences)\n",
        "\n",
        "        # Compile and export\n",
        "        model_template_JS_table = pd.DataFrame({\"model\": [model_name] * n, \"Template\": [template_index] * n, \"PosxPos\": pos_pos_JS_divergences, \"NegxNeg\": neg_neg_JS_divergences, \"PosxNeg\": pos_neg_JS_divergences})\n",
        "        JS_divergence_table = pd.concat([JS_divergence_table, model_template_JS_table])\n",
        "\n",
        "    # clean\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    gc.collect()\n",
        "    if is_torch_available():\n",
        "      torch.cuda.empty_cache()\n",
        "  \n",
        "  # output\n",
        "  return JS_divergence_table\n",
        "\n",
        "\n",
        "\n",
        "def compute_top_k_tokens_on_dataset(positive_pairs, negative_pairs, models_set, templates_set, k):\n",
        "\n",
        "  print(\"Device: \" + str(device))\n",
        "  batch_size = 64\n",
        "  max_length = 64\n",
        "\n",
        "  top_k_table = pd.DataFrame()\n",
        "\n",
        "  # For each model\n",
        "  for model_name, model_ref in models_set.items():\n",
        "    \n",
        "    print(\"Treating model \" + model_name)\n",
        "\n",
        "    # load model and tokenizer\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_ref)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ref, use_fast=True)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    mask_token_id = tokenizer.mask_token_id\n",
        "\n",
        "    # just inference, no backward needed\n",
        "    with torch.no_grad(): \n",
        "\n",
        "      for template_index in range(len(templates_set)):\n",
        "\n",
        "        print(\" - Template: \" + str(template_index))\n",
        "        \n",
        "        template = templates_set[template_index]\n",
        "\n",
        "        # load data for POSTIVE pairs\n",
        "        dataset_positive = ManualPromptDataset(data=positive_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n",
        "        dataloader_positive = DataLoader(dataset_positive, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "        # get tokens expected counts for positive examples\n",
        "        top_k_indexes_positives, top_k_e_counts_positives = get_expected_top_k(dataloader_positive, model, vocab_size, mask_token_id, k, device)\n",
        "        top_k_tokens_positives = [tokenizer.decode(t) for t in top_k_indexes_positives]\n",
        "\n",
        "        # load data for NEGATIVE pairs\n",
        "        dataset_negative = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n",
        "        dataloader_negative = DataLoader(dataset_negative, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "        # get tokens expected counts for negative examples\n",
        "        top_k_indexes_negatives, top_k_e_counts_negatives = get_expected_top_k(dataloader_negative, model, vocab_size, mask_token_id, k, device)\n",
        "        top_k_tokens_negatives = [tokenizer.decode(t) for t in top_k_indexes_negatives]\n",
        "\n",
        "        # Compile and export\n",
        "        n = k * 2\n",
        "        model_template_top_k_table = pd.DataFrame({\"model\": [model_name] * n, \"Template\": [template_index] * n, \"Type\": [\"Positive\"] * k + [\"Negative\"] * k, \"Rank\": list(range(1, k + 1)) * 2, \"index\": np.concatenate((top_k_indexes_positives, top_k_indexes_negatives)), \"word\": np.concatenate((top_k_tokens_positives, top_k_tokens_negatives)), \"count\": np.concatenate((top_k_e_counts_positives, top_k_e_counts_negatives))})\n",
        "        top_k_table = pd.concat([top_k_table, model_template_top_k_table])\n",
        "\n",
        "    # clean\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    gc.collect()\n",
        "    if is_torch_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  return top_k_table\n",
        "\n",
        "\n",
        "\n",
        "def compute_top1_correlation(positive_pairs, negative_pairs, models_set, templates_set):\n",
        "\n",
        "  print(\"Device: \" + str(device))\n",
        "  batch_size = 64\n",
        "  max_length = 64\n",
        "\n",
        "  top1_correlations = pd.DataFrame()\n",
        "  n_templates = len(templates_set)\n",
        "\n",
        "  # For each model\n",
        "  for model_name, model_ref in models_set.items():\n",
        "    \n",
        "    print(\"Treating model \" + model_name)\n",
        "\n",
        "    # load model and tokenizer\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_ref)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ref, use_fast=True)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    mask_token_id = tokenizer.mask_token_id\n",
        "\n",
        "    cors = np.empty(n_templates)\n",
        "\n",
        "    # just inference, no backward needed\n",
        "    with torch.no_grad(): \n",
        "\n",
        "      for template_index in range(len(templates_set)):\n",
        "\n",
        "        print(\" - Template: \" + str(template_index))\n",
        "        \n",
        "        template = templates_set[template_index]\n",
        "\n",
        "        # load data for POSTIVE pairs\n",
        "        dataset_positive = ManualPromptDataset(data=positive_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n",
        "        dataloader_positive = DataLoader(dataset_positive, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "        # get proba matrix for positive examples\n",
        "        proba_matrix_positives = get_proba_matrix(dataloader_positive, model, vocab_size, mask_token_id, device)\n",
        "\n",
        "        # load data for NEGATIVE pairs\n",
        "        dataset_negative = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n",
        "        dataloader_negative = DataLoader(dataset_negative, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "        # get proba matrix for negative examples\n",
        "        proba_matrix_negatives = get_proba_matrix(dataloader_negative, model, vocab_size, mask_token_id, device)\n",
        "\n",
        "        # Cpmpute top 1 correlations\n",
        "        top1_pos = get_top_1_distribution(proba_matrix_positives)\n",
        "        top1_neg = get_top_1_distribution(proba_matrix_negatives)\n",
        "        cors[template_index] = pearsonr(top1_pos, top1_neg)[0]\n",
        "\n",
        "    # Compile and export\n",
        "    model_template_cors = pd.DataFrame({\"model\": [model_name] * n_templates , \"Template\": list(range(n_templates)), \"cors\": cors})\n",
        "    top1_correlations = pd.concat([top1_correlations, model_template_cors])\n",
        "\n",
        "    # clean\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    gc.collect()\n",
        "    if is_torch_available():\n",
        "      torch.cuda.empty_cache()\n",
        "  \n",
        "  # output\n",
        "  return top1_correlations\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9jrYtHrgGJQU"
      },
      "source": [
        "## Evaluation 1: Can model distinguish true and false assertions about relations between fungi and natural products ?\n",
        "\n",
        "* 1) Compute the Jensen-Shanon divergences in $n=5000$ ramdomly sampled pairs of Postive-Negative examples, Positive-Positive examples, Negative-Negative examples.\n",
        "* 2) Extract the Top k predicted tokens expected counts\n",
        "* 3) Compute the top 1 predicted tken correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_dir = \"../../output/np/discriminant-analysis\" # to change depending on file system.\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SnRCGtEcETJ7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Treating model ChemicalBERT\n",
            " - Template: 0\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            " - Template: 1\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "Device: cpu\n",
            "Treating model ChemicalBERT\n",
            " - Template: 0\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            " - Template: 1\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n"
          ]
        }
      ],
      "source": [
        "##  Get JS - Divergences\n",
        "n_sample = 5000\n",
        "positive_pairs = data[data[\"nb_ref\"] > 0].reset_index()\n",
        "negative_pairs = data[data[\"nb_ref\"] < 0].reset_index()\n",
        "\n",
        "JS = compute_JS_divergences_on_dataset(positive_pairs, negative_pairs, models, templates_eval_1, n_sample, out_dir)\n",
        "JS.to_csv(os.path.join(out_dir, \"JS_divergence_eval_1_\" + str(n_sample) + \".tsv\"), index=False, sep=\"\\t\")\n",
        "\n",
        "##  Get Top k tokens\n",
        "k = 20\n",
        "TOP_K = compute_top_k_tokens_on_dataset(positive_pairs, negative_pairs, models, templates_eval_1, k)\n",
        "TOP_K.to_csv(os.path.join(out_dir, \"top_k_table_eval_1.tsv\"), index=False, sep=\"\\t\")\n",
        "\n",
        "## Get top 1 token correlation\n",
        "TOP_1_COR = compute_top1_correlation(positive_pairs, negative_pairs, models, templates_eval_1)\n",
        "TOP_1_COR.to_csv(os.path.join(out_dir, \"top_1_cor_eval_1.tsv\"), index=False, sep=\"\\t\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S3v2BgqUGcWM"
      },
      "source": [
        "## Evaluation 1: Can model distinguish true and false assertions about the antibiotic activity of a natural product ?\n",
        "\n",
        "* 1) Compute the Jensen-Shanon divergences in $n=5000$ ramdomly sampled pairs of Postive-Negative examples, Positive-Positive examples, Negative-Negative examples.\n",
        "* 2) Extract the Top k predicted tokens expected counts\n",
        "* 3) Compute the top 1 predicted tken correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4KSDIkjSGevC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Treating model ChemicalBERT\n",
            " - Template: 0\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            " - Template: 1\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "Device: cpu\n",
            "Treating model ChemicalBERT\n",
            " - Template: 0\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            " - Template: 1\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "Device: cpu\n",
            "Treating model ChemicalBERT\n",
            " - Template: 0\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            " - Template: 1\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n",
            "    - batch: 0\n",
            "    - batch: 1\n",
            "    - batch: 2\n",
            "    - batch: 3\n"
          ]
        }
      ],
      "source": [
        "##  Get JS - Divergences\n",
        "n_sample = 5000\n",
        "positive_pairs = data[data[\"y\"] == 1].reset_index()\n",
        "negative_pairs = data[data[\"y\"] == 0].reset_index()\n",
        "\n",
        "JS_2 = compute_JS_divergences_on_dataset(positive_pairs, negative_pairs, models, templates_eval_2, n_sample, out_dir)\n",
        "JS_2.to_csv(os.path.join(out_dir, \"JS_divergence_eval_2_\" + str(n_sample) + \".tsv\"), index=False, sep=\"\\t\")\n",
        "\n",
        "##  Get Top k tokens\n",
        "k = 20\n",
        "TOP_K = compute_top_k_tokens_on_dataset(positive_pairs, negative_pairs, models, templates_eval_2, k)\n",
        "TOP_K.to_csv(os.path.join(out_dir, \"top_k_table_eval_2.tsv\"), index=False, sep=\"\\t\")\n",
        "\n",
        "TOP_1_COR = compute_top1_correlation(positive_pairs, negative_pairs, models, templates_eval_2)\n",
        "TOP_1_COR.to_csv(os.path.join(out_dir, \"top_1_cor_eval_2.tsv\"), index=False, sep=\"\\t\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "prompting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "23662edbd0fa380e56cbb9afccf816d63aee52f726084a08b71fbb21b54fc616"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
